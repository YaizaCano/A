\documentclass{article}


\title{Hashing and Bloom filters key lookup comparison}
\author{names}
\date{\today}


\usepackage[backend=bibtex]{biblatex}
\usepackage{graphicx}
\usepackage{amsmath}


\bibliography{references}
\nocite{*}

\begin{document}
    \maketitle
    \thispagestyle{empty}
    \begin{abstract}

    \end{abstract}


    \section{Introduction}
        Hashing methods and probabilistic data structures such as Bloom filters have and increasing use nowadays with the necessity of 
        fast and efficient data structures for dictionary abstraction. Dictionaries provide use with a way to organize data and enable 
        a fast search and manipulation. In this report we wish to compare different different hash table implementations and bloom filters 
        in the task of key lookup in the dictionary. In this research we compare three types of dictionaries implementations Open Addressing Tables, 
   	Separate Chaining Tables and Bloom Filters. 

    \section{Experiment pipeline and methodology}
    	The experimentation process was a very tedious task since it required to take in account a lot of parameters. An experiment is divided into two parts the table build, where every $n$ keys of a dictionary are added, and the table lookup where $2 * n + n * keyPercenatge$ keys are searched in each type of hash table and the bloom filters. In the lookup part $2 * n$ keys of the search are not in the table and should return an unsuccessful search, the rest of the keys should return a successful search. 
    	to the table and the table lookup, where given a series of keys 
    	.....
    	\begin{center}
	 \begin{tabular}{||c | c ||} 
	 \hline
	 \multicolumn{2}{|c|}{Parameters List} \\
	 \hline\hline 
	 Parameter & Definition \\
	\hline
	 $n$ & This is the number of keys that where inserted into the table \\
	 \hline
	 $m$ & This is the size of the table \\
	 \hline
	 $\alpha$ & The table load factor ($\frac{n}{m}$) \\
	 \hline
	 $k$ & Number of hash functions to use for the bloom filters \\
	 \hline
	 $keyPercentage$ & The percentage of keys that appear in the text files \\
	 \hline
	 $seed$ & The initial number for the random integer generation \\
	 \hline
	 $maxLoop$ & Max recursive probes for the Cuckoo hashing \\
	 \hline
	\end{tabular}
	\end{center}



    \section{Data Generation}
        The data used in each of the experiments consisted of a sequence of unsigned integers,  
        the data was written in two different files, to distinguish them we used a specific 
        format specified in figure X. One file contained the keys to insert into the dictionary and the other the text to find in the dictionary. 
        The keys file contained n numbers and the text contained $2 * n + p * n$ where $p$ is the percentage of keys that are already inserted in the dictionary.
		\subsection*{Linear Congruential Method}
		The randomness of the keys was based on the Linear congruential method (LCM). 
		We needed to have experiments as reliable as possible thus we needed a way to control the data generation, 
		that is why we used this method. The LCM is one of the oldest and best-known random number generators, 
		it is also very easy to implement. This method generates a cyclic sequence of numbers based on an initial seed. 
		Given a seed $X_0$ it is possible to generate the next number in the sequence 
		\begin{equation}
		X_i = (X_{i-1} * a + c) \, mod m
		\end{equation}
		 where $a$ is called the multiplier, $c$ the incrementer and $m$ the modulus. 
		 The size of the sequence depends on the parameters used, independently on the initial seed.
		  The best parameters are the ones that yield the maximum size sequence, that is parameters that yield $m$ numbers.  
		  The parameters that we used where ...
    \section{Experiment parameters}

    \section{Results}

    \section{Discussion}



    \printbibliography

\end{document}
